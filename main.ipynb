{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:22:19.438760: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-10 09:22:19.463538: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-10 09:22:19.471153: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 09:22:19.490623: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-10 09:22:21.994255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Add the absolute path to the root directory of the project\n",
    "sys.path.append(\"/cs/cs_groups/cliron_group/Calibrato\")\n",
    "import time\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import logging\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from calibrators.geometric_calibrators import GeometricCalibrator, GeometricCalibratorTrust\n",
    "from utils.logging_config import setup_logging\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import torch\n",
    "from torchvision.models import DenseNet\n",
    "import joblib\n",
    "from models.model_factory import get_model  # Import the factory function\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "from utils.metrics import CalibrationMetrics\n",
    "from utils.utils import StabilitySpace, Compression\n",
    "import random\n",
    "from skimage.transform import rotate\n",
    "from scipy.ndimage import shift\n",
    "# from calibrators.calibrators import *\n",
    "from calibrators.ensemble_calibrators import *\n",
    "from calibrators.non_parametric_calibrators import *\n",
    "from calibrators.parametric_calibrators import *\n",
    "from calibrators.specialized_calibrators import *\n",
    "from calibrators.trust_score_calibration import *\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from filelock import FileLock\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logging\n",
    "setup_logging()\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def transform_test_set(X_test, transform_ratios=(0.3, 0.3, 0.4), random_state=None):\n",
    "    \"\"\"\n",
    "    Apply transformations to the test set: rotate, shift, and add noise.\n",
    "\n",
    "    Args:\n",
    "        X_test (numpy.ndarray): Test set images.\n",
    "        transform_ratios (tuple): Ratios for (rotation, shift, noise).\n",
    "        random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Transformed test set.\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    n_samples = len(X_test)\n",
    "    n_rotate = int(n_samples * transform_ratios[0])\n",
    "    n_shift = int(n_samples * transform_ratios[1])\n",
    "    n_noise = n_samples - n_rotate - n_shift\n",
    "\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    # Get subsets\n",
    "    rotate_indices = indices[:n_rotate]\n",
    "    shift_indices = indices[n_rotate:n_rotate + n_shift]\n",
    "    noise_indices = indices[n_rotate + n_shift:]\n",
    "\n",
    "    # Apply transformations\n",
    "    transformed_X_test = X_test.copy()\n",
    "\n",
    "    # Rotation\n",
    "    for idx in rotate_indices:\n",
    "        angle = random.uniform(-30, 30)  # Rotate by a random angle between -30 and 30 degrees\n",
    "        transformed_X_test[idx] = rotate(transformed_X_test[idx], angle, mode='wrap')\n",
    "\n",
    "    # Shifting\n",
    "    for idx in shift_indices:\n",
    "        shift_x = random.uniform(-5, 5)  # Shift up to Â±5 pixels\n",
    "        shift_y = random.uniform(-5, 5)\n",
    "        transformed_X_test[idx] = shift(transformed_X_test[idx], shift=(shift_x, shift_y, 0), mode='wrap')\n",
    "\n",
    "    # Noise\n",
    "    for idx in noise_indices:\n",
    "        noise = np.random.random(transformed_X_test[idx].shape) * 0.4  # Add up to 40% noise\n",
    "        transformed_X_test[idx] = np.clip(transformed_X_test[idx] + noise, 0, 1)\n",
    "\n",
    "    return transformed_X_test\n",
    "\n",
    "\n",
    "def build_densenet40_model(num_classes=10):\n",
    "    model = DenseNet(\n",
    "        growth_rate=12,\n",
    "        block_config=(6, 6, 6),\n",
    "        num_init_features=24,\n",
    "        bn_size=4,\n",
    "        drop_rate=0,\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_and_split_data(dataset_name, random_state):\n",
    "    \"\"\"\n",
    "    Load and split data into train, validation, and test sets.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (str): Name of the dataset to load ('MNIST', 'Fashion MNIST', 'CIFAR-10', 'CIFAR-100', 'Tiny ImageNet').\n",
    "        random_state (int): Random state for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: (X_train, X_val, X_test, y_train, y_val, y_test)\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading dataset: {dataset_name}\")\n",
    "    \n",
    "    # Load dataset based on the given name\n",
    "    if dataset_name.lower() == \"mnist\":\n",
    "        (train_X_original, train_y_original), (test_X_original, test_y_original) = tf.keras.datasets.mnist.load_data()\n",
    "        input_shape = (28, 28, 1)\n",
    "    \n",
    "    elif dataset_name.lower() == \"fashion_mnist\":\n",
    "        (train_X_original, train_y_original), (test_X_original, test_y_original) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "        input_shape = (28, 28, 1)\n",
    "    \n",
    "    elif dataset_name.lower() == \"cifar10\":\n",
    "        (train_X_original, train_y_original), (test_X_original, test_y_original) = tf.keras.datasets.cifar10.load_data()\n",
    "        input_shape = (32, 32, 3)\n",
    "    \n",
    "    elif dataset_name.lower() == \"cifar100\":\n",
    "        (train_X_original, train_y_original), (test_X_original, test_y_original) = tf.keras.datasets.cifar100.load_data()\n",
    "        input_shape = (32, 32, 3)\n",
    "\n",
    "    elif dataset_name.lower() == \"tiny_imagenet\":\n",
    "        logging.info(\"Loading Tiny ImageNet dataset\")\n",
    "        splits = {\n",
    "            'train': 'data/train-00000-of-00001-1359597a978bc4fa.parquet',\n",
    "            'valid': 'data/valid-00000-of-00001-70d52db3c749a935.parquet'\n",
    "        }\n",
    "        \n",
    "        # Load training and validation data\n",
    "        train_df = pd.read_parquet(\"hf://datasets/zh-plus/tiny-imagenet/\" + splits['train'])\n",
    "        valid_df = pd.read_parquet(\"hf://datasets/zh-plus/tiny-imagenet/\" + splits['valid'])\n",
    "        \n",
    "        logging.info(f\"Train DataFrame head:\\n{train_df.head()}\")\n",
    "        logging.info(f\"Validation DataFrame head:\\n{valid_df.head()}\")\n",
    "        \n",
    "        # Decode images from binary data\n",
    "        def decode_image(row):\n",
    "            binary_data = row['bytes']  # Extract binary data\n",
    "            try:\n",
    "                image = Image.open(io.BytesIO(binary_data))  # Decode the image\n",
    "                image = image.convert(\"RGB\")  # Ensure RGB\n",
    "                image = image.resize((64, 64))  # Resize to 64x64\n",
    "                return np.array(image)  # Convert to numpy array\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to decode or resize image: {e}\")\n",
    "                return None\n",
    "\n",
    "        # Decode and filter valid images\n",
    "        train_X_original = np.stack([\n",
    "            img for img in (decode_image(img) for img in train_df['image']) if img is not None\n",
    "        ])\n",
    "        test_X_original = np.stack([\n",
    "            img for img in (decode_image(img) for img in valid_df['image']) if img is not None\n",
    "        ])\n",
    "        logging.info(f\"Shape of train_X_original: {train_X_original.shape}\")\n",
    "        logging.info(f\"Shape of test_X_original: {test_X_original.shape}\")\n",
    "\n",
    "        train_y_original = np.array(train_df['label'])\n",
    "        test_y_original = np.array(valid_df['label'])\n",
    "\n",
    "        # Set random seed for reproducibility (optional)\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        # Calculate the number of samples for half the dataset\n",
    "        num_samples_train = train_X_original.shape[0] // 4\n",
    "        num_samples_test = test_X_original.shape[0] // 2\n",
    "\n",
    "        # Generate random indices to sample\n",
    "        random_indices_train = np.random.choice(train_X_original.shape[0], size=num_samples_train, replace=False)\n",
    "        random_indices_test = np.random.choice(test_X_original.shape[0], size=num_samples_test, replace=False)\n",
    "\n",
    "        # Sample the images and labels using the random indices\n",
    "        train_X_original = train_X_original[random_indices_train]\n",
    "        train_y_original = train_y_original[random_indices_train]\n",
    "        test_X_original = test_X_original[random_indices_test]\n",
    "        test_y_original = test_y_original[random_indices_test]\n",
    "\n",
    "        logging.info(f\"Shape of sampled train_X: {train_X_original.shape}, train_y: {train_y_original.shape}\")\n",
    "        logging.info(f\"Shape of sampled test_X: {test_X_original.shape}, test_y: {test_y_original.shape}\")\n",
    "\n",
    "        \n",
    "        input_shape = (64, 64, 3)  # Tiny ImageNet images are 64x64 RGB\n",
    "        logging.info(\"Tiny ImageNet loaded successfully\")\n",
    "\n",
    "\n",
    "    elif dataset_name.lower() == \"GTSRB\":\n",
    "        # Placeholder for loading GTSRB (German Traffic Sign Benchmark)\n",
    "        # Replace this with actual GTSRB loading code, e.g., from a local or custom dataset loader.\n",
    "        # Example:\n",
    "        # (train_X_original, train_y_original), (test_X_original, test_y_original) = load_GTSRB()\n",
    "        raise NotImplementedError(\"GTSRB dataset loading not implemented. Use an appropriate data loader.\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Dataset '{dataset_name}' not recognized. Choose from 'MNIST', 'Fashion MNIST', 'CIFAR-10', 'CIFAR-100', or 'GTRSB'.\")\n",
    "\n",
    "    logging.info(\"Combining and splitting data\")\n",
    "    \n",
    "    # Combine train and test data for further splitting\n",
    "    data = np.concatenate((train_X_original, test_X_original), axis=0)\n",
    "    labels = np.concatenate((train_y_original, test_y_original), axis=0).squeeze()  # Ensure labels are 1D for compatibility\n",
    "    logging.debug(f\"Data shape: {data.shape}, Labels shape: {labels.shape}\")\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=random_state)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=random_state)\n",
    "    logging.debug(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "    # Expand dimensions if necessary for grayscale images\n",
    "    if input_shape[-1] == 1:  # Grayscale datasets (MNIST, Fashion MNIST)\n",
    "        logging.info(\"Expanding dimensions for grayscale images\")\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "        X_val = np.expand_dims(X_val, axis=-1)\n",
    "\n",
    "    # Ensure data is cast to float32 for neural network compatibility\n",
    "    X_train, X_val, X_test = X_train.astype(\"float32\"), X_val.astype(\"float32\"), X_test.astype(\"float32\")\n",
    "    X_train /= 255.0\n",
    "    X_val /= 255.0\n",
    "    X_test /= 255.0\n",
    "    logging.info(\"Data normalization completed\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "\n",
    "# Adjusted function to train or load the specified model type\n",
    "def train_or_load_model(X_train, y_train, X_val, y_val, dataset_name, random_state, model_type=\"cnn\", file_format=\"keras\", epochs=None, epochs_dict=None):\n",
    "    \"\"\"\n",
    "    Train or load a specified model type, with dynamic number of epochs based on dataset size.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training data.\n",
    "        y_train: Training labels.\n",
    "        X_val: Validation data.\n",
    "        y_val: Validation labels.\n",
    "        dataset_name (str): Name of the dataset, used for directory structure.\n",
    "        random_state (int): Random state number, used for directory structure.\n",
    "        model_type (str): Type of model (e.g., \"cnn\").\n",
    "        file_format (str): File format for saving model, options are \"keras\" or \"h5\".\n",
    "        epochs (int, optional): If specified, use this number of epochs. Otherwise, it is dynamically calculated.\n",
    "        epochs_dict (dict, optional): Dictionary mapping dataset names to specific epochs.\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained or loaded model.\n",
    "    \"\"\"\n",
    "    model_directory = f\"{dataset_name}/{random_state}/saved_models\"\n",
    "    model_path = os.path.join(model_directory, f\"{model_type}_model.{file_format}\")\n",
    "    lock_path = f\"{model_path}.lock\"\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    input_shape = X_train.shape[1:]\n",
    "\n",
    "    # Determine the number of epochs\n",
    "    if epochs_dict and dataset_name.lower() in epochs_dict:\n",
    "        epochs = epochs_dict[dataset_name.lower()]\n",
    "    elif epochs is None:\n",
    "        # Dynamically calculate epochs based on the dataset size\n",
    "        epochs = max(1, len(X_train) // 800)  # At least 1 epoch for very small datasets\n",
    "    epochs = int(epochs)  # Ensure epochs is an integer\n",
    "\n",
    "    logger.info(f\"Using {epochs} epochs for training.\")\n",
    "\n",
    "    # Preprocess data for ResNet if needed\n",
    "    if model_type == \"pretrained_resnet\":\n",
    "        batch = 16\n",
    "    else:\n",
    "        batch = 32\n",
    "        \n",
    "    # Use a file lock to handle concurrent access\n",
    "    with FileLock(lock_path):\n",
    "        # Check if the model already exists\n",
    "        if os.path.exists(model_path):\n",
    "            logger.info(f\"Loading pre-trained {model_type} model from {model_path}.\")\n",
    "            if model_type in [\"cnn\", \"pretrained_resnet\"]:\n",
    "                model = tf.keras.models.load_model(model_path)\n",
    "            else:\n",
    "                model = joblib.load(model_path)\n",
    "        else:\n",
    "            # Train the model\n",
    "            logger.info(f\"Training new {model_type} model.\")\n",
    "            model = get_model(dataset_name, model_type, input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "            if model_type in [\"cnn\", \"pretrained_resnet\"]:\n",
    "                model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch, verbose=2)\n",
    "                os.makedirs(model_directory, exist_ok=True)\n",
    "                model.save(model_path)\n",
    "            else:\n",
    "                model.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
    "                os.makedirs(model_directory, exist_ok=True)\n",
    "                joblib.dump(model, model_path)\n",
    "\n",
    "            logger.info(f\"Model saved at {model_path}.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def calibrate_with_geometric(model, X_train, y_train, X_val, y_val, X_test, y_test, library, metric='l2'):\n",
    "    \"\"\"\n",
    "    Apply geometric calibration with the specified library (FAISS, KNN, or separation).\n",
    "    \n",
    "    Args:\n",
    "        model: The model to calibrate\n",
    "        X_train, y_train: Training data\n",
    "        X_val, y_val: Validation data\n",
    "        X_test, y_test: Test data\n",
    "        library: Library to use for stability calculation\n",
    "        metric: Distance metric to use (default: 'l2')\n",
    "    \"\"\"\n",
    "    geo_calibrator = GeometricCalibrator(\n",
    "        model=model, \n",
    "        X_train=X_train, \n",
    "        y_train=y_train, \n",
    "        library=library,\n",
    "        metric=metric\n",
    "    )\n",
    "    geo_calibrator.fit(X_val, y_val)\n",
    "\n",
    "    # Calibrate the test set\n",
    "    calibrated_probs = geo_calibrator.calibrate(X_test)\n",
    "    y_test_pred = np.argmax(calibrated_probs, axis=1)\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    logger.info(f\"Accuracy after calibration with {library} using {metric} metric: {accuracy}\")\n",
    "\n",
    "    return calibrated_probs, y_test_pred\n",
    "\n",
    "\n",
    "def calculate_ece(probs, y_pred, y_true, n_bins=20):\n",
    "    \"\"\"\n",
    "    Calculate Expected Calibration Error (ECE).\n",
    "    \"\"\"\n",
    "    confidence_of_pred_class = np.max(probs, axis=1)\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_indices = np.digitize(confidence_of_pred_class, bin_boundaries) - 1\n",
    "\n",
    "    total_error = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_mask = bin_indices == i\n",
    "        bin_confidences = confidence_of_pred_class[bin_mask]\n",
    "        bin_real = y_true[bin_mask]\n",
    "        bin_pred = y_pred[bin_mask]\n",
    "\n",
    "        if len(bin_confidences) > 0:\n",
    "            bin_acc = np.mean(bin_real == bin_pred)\n",
    "            bin_conf = np.mean(bin_confidences)\n",
    "            bin_weight = len(bin_confidences) / len(probs)\n",
    "            total_error += bin_weight * np.abs(bin_acc - bin_conf)\n",
    "\n",
    "    logger.info(f\"Final ECE value: {total_error}\")\n",
    "    return total_error\n",
    "\n",
    "\n",
    "def initialize_directories(base_dir, transformed, dataset_name, random_state, model_type, metric, trust_alpha):\n",
    "    if transformed:\n",
    "        base_dir = base_dir + \"/transformed\"\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    technique_dirs = {\n",
    "        \"faiss_exact\": os.path.join(base_dir, \"faiss_exact\"),\n",
    "        \"knn\": os.path.join(base_dir, \"knn\"),\n",
    "        \"separation\": os.path.join(base_dir, \"separation\"),\n",
    "        \"isotonic\": os.path.join(base_dir, \"isotonic\"),\n",
    "        \"platt\": os.path.join(base_dir, \"platt\"),\n",
    "        \"temperature\": os.path.join(base_dir, \"temperature\"),\n",
    "        \"trust_score_filtered\": os.path.join(base_dir, f\"trust_score_filtered\"),\n",
    "        \"trust_score_unfiltered\": os.path.join(base_dir, f\"trust_score_unfiltered\"),\n",
    "    }\n",
    "\n",
    "    all_results_dir = os.path.join(base_dir, \"all\")\n",
    "    os.makedirs(all_results_dir, exist_ok=True)\n",
    "\n",
    "    for directory in technique_dirs.values():\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    return base_dir, technique_dirs, all_results_dir\n",
    "\n",
    "\n",
    "def prepare_data(dataset_name, random_state, transformed):\n",
    "    \"\"\"Load and optionally transform dataset.\"\"\"\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = load_and_split_data(dataset_name, random_state)\n",
    "    if transformed:\n",
    "        X_test = transform_test_set(X_test)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def prepare_model(X_train, y_train, X_val, y_val, dataset_name, random_state, model_type, dataset_epochs):\n",
    "    \"\"\"Train or load the model.\"\"\"\n",
    "    return train_or_load_model(\n",
    "        X_train, y_train, X_val, y_val,\n",
    "        dataset_name=dataset_name,\n",
    "        random_state=random_state,\n",
    "        model_type=model_type,\n",
    "        epochs_dict=dataset_epochs\n",
    "    )\n",
    "\n",
    "def calibrate_geometric(model, X_train, y_train, X_val, y_val, X_test, y_test, calibrator, name, technique_dirs, metric, use_binning, compression=None):\n",
    "    \"\"\"\n",
    "    Perform geometric calibrations (FAISS, KNN, Separation) with optional binning.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Apply compression if provided\n",
    "        stability_space = StabilitySpace(\n",
    "            X_train, \n",
    "            y_train, \n",
    "            compression=compression,\n",
    "            library=calibrator[\"library\"],\n",
    "            faiss_mode=calibrator.get(\"mode\"), \n",
    "            metric=metric\n",
    "        )\n",
    "\n",
    "        geo_calibrator = GeometricCalibrator(\n",
    "            model=model,\n",
    "            X_train=X_train,\n",
    "            y_train=y_train,\n",
    "            stability_space=stability_space,\n",
    "            library=calibrator[\"library\"],\n",
    "            metric=metric,\n",
    "            use_binning=use_binning,  # Enable or disable binning\n",
    "        )\n",
    "        geo_calibrator.fit(X_val, y_val)\n",
    "        calibrated_probs = geo_calibrator.calibrate(X_test)\n",
    "        y_test_pred_cal = np.argmax(calibrated_probs, axis=1)\n",
    "\n",
    "        metrics = CalibrationMetrics(calibrated_probs, y_test_pred_cal, y_test, n_bins=20)\n",
    "        metrics_dict = metrics.calculate_all_metrics()\n",
    "        calibration_time = time.time() - start_time\n",
    "\n",
    "        results = {\n",
    "            \"Metric\": f\"{name.replace('_', ' ').capitalize()}\",\n",
    "            **metrics_dict,\n",
    "            \"Calibration Time (s)\": calibration_time,\n",
    "        }\n",
    "\n",
    "        results_csv_file = os.path.join(technique_dirs[name], \"results.csv\")\n",
    "        pd.DataFrame([results]).to_csv(results_csv_file, index=False)\n",
    "        print(f\"{name.replace('_', ' ').capitalize()} Metrics saved to {results_csv_file}\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calibrating with {name}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def calibrate_parametric(features_val, y_val, features_test, y_test, calibrator, name, technique_dirs):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Starting parametric calibration with {name}.\")\n",
    "        calibrator.fit(features_val, y_val)\n",
    "        calibrated_probs = calibrator.calibrate(features_test)\n",
    "\n",
    "        # Ensure calibrated_probs is 2D\n",
    "        if calibrated_probs.ndim == 1:\n",
    "            # Convert to 2D array with two classes (binary classification)\n",
    "            calibrated_probs = np.vstack([1 - calibrated_probs, calibrated_probs]).T\n",
    "\n",
    "        y_test_pred_cal = np.argmax(calibrated_probs, axis=1)\n",
    "\n",
    "        metrics = CalibrationMetrics(calibrated_probs, y_test_pred_cal, y_test, n_bins=20)\n",
    "        metrics_dict = metrics.calculate_all_metrics()\n",
    "        calibration_time = time.time() - start_time\n",
    "\n",
    "        results = {\n",
    "            \"Metric\": name.replace(\"_\", \" \").capitalize(),\n",
    "            **metrics_dict,\n",
    "            \"Calibration Time (s)\": calibration_time,\n",
    "        }\n",
    "\n",
    "        results_csv_file = os.path.join(technique_dirs[name], \"results.csv\")\n",
    "        pd.DataFrame([results]).to_csv(results_csv_file, index=False)\n",
    "        print(f\"{name.replace('_', ' ').capitalize()} Metrics saved to {results_csv_file}\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calibrating with {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# def calibrate_trust_score(X_train, y_train, X_test, y_test_pred, y_test, trust_alpha, technique_dirs, use_filtering=True):\n",
    "#     try:\n",
    "#         start_time = time.time()\n",
    "#         filter_mode = \"density\" if use_filtering else \"none\"\n",
    "#         logger.info(f\"Starting Trust Score calibration with filtering: {filter_mode}\")\n",
    "\n",
    "#         # Ensure consistent dimensions\n",
    "#         X_train_flat = X_train.reshape(X_train.shape[0], -1) if len(X_train.shape) > 2 else X_train\n",
    "#         X_test_flat = X_test.reshape(X_test.shape[0], -1) if len(X_test.shape) > 2 else X_test\n",
    "\n",
    "#         # Fit Trust Score Calibrator\n",
    "#         trust_score_calibrator = TrustScoreCalibrator(k=10, alpha=trust_alpha, filtering=filter_mode)\n",
    "#         trust_score_calibrator.fit(X_train_flat, y_train)\n",
    "#         calibrated_probs = trust_score_calibrator.calibrate(X_test_flat, y_test_pred)\n",
    "\n",
    "#         # Evaluate calibrated results\n",
    "#         y_test_pred_cal = np.argmax(calibrated_probs, axis=1)\n",
    "#         metrics = CalibrationMetrics(calibrated_probs, y_test_pred_cal, y_test, n_bins=20)\n",
    "#         metrics_dict = metrics.calculate_all_metrics()\n",
    "\n",
    "#         calibration_time = time.time() - start_time\n",
    "#         results = {\n",
    "#             \"Metric\": f\"Trust Score Geometric {'Filtered' if use_filtering else 'Unfiltered'}\",\n",
    "#             **metrics_dict,\n",
    "#             \"Calibration Time (s)\": calibration_time,\n",
    "#         }\n",
    "\n",
    "#         # Save results\n",
    "#         filter_tag = \"filtered\" if use_filtering else \"unfiltered\"\n",
    "#         results_csv_file = os.path.join(technique_dirs[f\"trust_score_{filter_tag}\"], \"results.csv\")\n",
    "#         pd.DataFrame([results]).to_csv(results_csv_file, index=False)\n",
    "#         print(f\"Trust Score {filter_tag.capitalize()} Metrics saved to {results_csv_file}\")\n",
    "#         return results\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error during Trust Score calibration ({filter_mode}): {e}\")\n",
    "#         return None\n",
    "\n",
    "def calibrate_trust_score(model, X_train, y_train, X_val, y_val, X_test, y_test, trust_alpha, \n",
    "                         technique_dirs, use_filtering=True, use_binning=True, n_bins=50):\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        logger.info(f\"Starting Geometric Trust Score calibration with binning: {use_binning}\")\n",
    "\n",
    "        # Ensure consistent dimensions\n",
    "        X_train_flat = X_train.reshape(X_train.shape[0], -1) if len(X_train.shape) > 2 else X_train\n",
    "        X_val_flat = X_val.reshape(X_val.shape[0], -1) if len(X_val.shape) > 2 else X_val\n",
    "        X_test_flat = X_test.reshape(X_test.shape[0], -1) if len(X_test.shape) > 2 else X_test\n",
    "\n",
    "        # Initialize and fit Geometric Trust Score Calibrator\n",
    "        geometric_trust = GeometricCalibratorTrust(\n",
    "            model=model,\n",
    "            X_train=X_train_flat,\n",
    "            y_train=y_train,\n",
    "            k=10,\n",
    "            min_dist=1e-12,\n",
    "            use_binning=use_binning,\n",
    "            n_bins=n_bins,\n",
    "            use_filtering=use_filtering,\n",
    "            alpha=trust_alpha\n",
    "        )\n",
    "\n",
    "\n",
    "        # Fit calibrator using validation set\n",
    "        geometric_trust.fit(X_val_flat, y_val)\n",
    "\n",
    "        # Get calibrated probabilities\n",
    "        calibrated_probs = geometric_trust.calibrate(X_test_flat)\n",
    "\n",
    "        # Evaluate calibrated results\n",
    "        y_test_pred_cal = np.argmax(calibrated_probs, axis=1)\n",
    "        metrics = CalibrationMetrics(calibrated_probs, y_test_pred_cal, y_test, n_bins=20)\n",
    "        metrics_dict = metrics.calculate_all_metrics()\n",
    "\n",
    "        calibration_time = time.time() - start_time\n",
    "        results = {\n",
    "            \"Metric\": f\"Geometric Trust Score {'Binned' if use_binning else 'Unbinned'}\",\n",
    "            **metrics_dict,\n",
    "            \"Calibration Time (s)\": calibration_time,\n",
    "        }\n",
    "\n",
    "        # Save results\n",
    "        binning_tag = \"binned\" if use_binning else \"unbinned\"\n",
    "        results_csv_file = os.path.join(technique_dirs[f\"geometric_trust_{binning_tag}\"], \"results.csv\")\n",
    "        pd.DataFrame([results]).to_csv(results_csv_file, index=False)\n",
    "        print(f\"Geometric Trust Score {binning_tag.capitalize()} Metrics saved to {results_csv_file}\")\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during Geometric Trust Score calibration: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "\n",
    "def compute_uncalibrated_metrics(features_test, y_test_pred, y_test, train_size, val_size, test_size):\n",
    "    \"\"\"Compute metrics for the uncalibrated model.\"\"\"\n",
    "    metrics_uncalibrated = CalibrationMetrics(features_test, y_test_pred, y_test, n_bins=20)\n",
    "    metrics_dict = metrics_uncalibrated.calculate_all_metrics()\n",
    "    results = {\n",
    "        \"Metric\": \"Uncalibrated\",\n",
    "        **metrics_dict,\n",
    "        \"Calibration Time (s)\": \"N/A\",\n",
    "        \"Train Size\": train_size,\n",
    "        \"Validation Size\": val_size,\n",
    "        \"Test Size\": test_size\n",
    "    }\n",
    "    print(f\"Uncalibrated Metrics: {metrics_dict}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def save_results(results, all_results_dir):\n",
    "    \"\"\"Save all results to a CSV file.\"\"\"\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_csv_file = os.path.join(all_results_dir, \"all_results.csv\")\n",
    "    results_df.to_csv(results_csv_file, index=False)\n",
    "    print(f\"All results saved to {results_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset_name, random_state, model_type=\"cnn\", metric=\"L2\", transformed=False, trust_alpha=0.1,\n",
    "         compression_types=None, compression_params=2):\n",
    "    # Initialize directories\n",
    "    base_dir, technique_dirs, all_results_dir = initialize_directories(\n",
    "        f\"/cs/cs_groups/cliron_group/Calibrato/{dataset_name}/{random_state}/{model_type}/{metric}\",\n",
    "        transformed, dataset_name, random_state, model_type, metric, trust_alpha\n",
    "    )\n",
    "    compression = None\n",
    "    if compression_types:\n",
    "        logger.info(f\"Initializing Compression with types: {compression_types}, params: {compression_params}\")\n",
    "        compression = Compression(compression_types=compression_types, compression_params=compression_params)\n",
    "\n",
    "    # Load data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(dataset_name, random_state, transformed)\n",
    "\n",
    "    # Train or load model\n",
    "    dataset_epochs = {\n",
    "        \"mnist\": 10,\n",
    "        \"fashion_mnist\": 20,\n",
    "        \"cifar10\": 35,\n",
    "        \"cifar100\": 100,\n",
    "        \"sign_language\": 25,\n",
    "        \"tiny_imagenet\": 30,\n",
    "    }\n",
    "    model = prepare_model(X_train, y_train, X_val, y_val, dataset_name, random_state, model_type, dataset_epochs)\n",
    "\n",
    "    # Get dataset sizes\n",
    "    train_size, val_size, test_size = len(X_train), len(X_val), len(X_test)\n",
    "\n",
    "    # Preprocess data based on the model type\n",
    "    if model_type in [\"cnn\", \"densenet\", \"pretrained_resnet\"]:\n",
    "        # For TensorFlow/Keras models, no additional preprocessing is needed\n",
    "        logger.info(f\"Using TensorFlow/Keras model: {model_type}\")\n",
    "        features_test = model.predict(X_test)\n",
    "        y_test_pred = np.argmax(features_test, axis=1)\n",
    "        features_val = model.predict(X_val)\n",
    "        y_val_pred = np.argmax(features_val, axis=1)\n",
    "    else:\n",
    "        # For sklearn models, flatten the input data to 2D\n",
    "        logger.info(f\"Using sklearn model: {model_type}\")\n",
    "        X_test = X_test.reshape(X_test.shape[0], -1) if len(X_test.shape) > 2 else X_test\n",
    "        X_val = X_val.reshape(X_val.shape[0], -1) if len(X_val.shape) > 2 else X_val\n",
    "\n",
    "        features_test = model.predict_proba(X_test)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        features_val = model.predict_proba(X_val)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "    logger.info(f\"Feature extraction complete for model type: {model_type}\")\n",
    "    # Uncalibrated metrics\n",
    "    results = []\n",
    "    uncalibrated_results = compute_uncalibrated_metrics(features_test, y_test_pred, y_test, train_size, val_size, test_size)\n",
    "    results.append(uncalibrated_results)\n",
    "\n",
    "    # Define calibration methods\n",
    "    calibrations = {\n",
    "        \"isotonic\": IsotonicCalibrator(),\n",
    "        \"platt\": PlattCalibrator(),\n",
    "        \"temperature\": TemperatureScalingCalibrator(),\n",
    "        # \"trust_score_filtered\": {\"method\": \"trust_score\", \"use_filtering\": True},\n",
    "        \"trust_score_unfiltered\": {\"method\": \"trust_score\", \"use_filtering\": False},\n",
    "        \"faiss_exact\": {\"library\": \"faiss\", \"mode\": \"exact\"},\n",
    "        \"faiss_binned\": {\"library\": \"faiss\", \"mode\": \"exact\", \"binned\": True},\n",
    "        \"knn\": {\"library\": \"knn\", \"mode\": None},\n",
    "        \"knn_binned\": {\"library\": \"knn\", \"mode\": None, \"binned\": True},\n",
    "    }\n",
    "\n",
    "\n",
    "    # Add faiss_approximate if applicable\n",
    "    if dataset_name.lower() not in [\"cifar100\", \"tiny_imagenet\"]:\n",
    "        calibrations[\"faiss_approximate\"] = {\"library\": \"faiss\", \"mode\": \"approximate\"}\n",
    "        calibrations[\"trust_score_unfiltered\"] = {\"method\": \"trust_score\", \"use_filtering\": False}\n",
    "        calibrations[\"trust_score_filtered\"] = {\"method\": \"trust_score\", \"use_filtering\": True}\n",
    "\n",
    "    # Loop through calibration methods\n",
    "    for name, calibrator in calibrations.items():\n",
    "        # Initialize variables at the start of each iteration\n",
    "        trust_results = None\n",
    "        parametric_results = None\n",
    "        geometric_results = None\n",
    "\n",
    "        if name.startswith(\"trust_score\"):\n",
    "            # Trust Score calibrations\n",
    "            use_filtering = calibrator[\"use_filtering\"]\n",
    "            trust_results = calibrate_trust_score(\n",
    "                model=model,  # Add model parameter\n",
    "                X_train=X_train,\n",
    "                y_train=y_train,\n",
    "                X_val=X_val,  # Add validation data\n",
    "                y_val=y_val,  # Add validation labels\n",
    "                X_test=X_test,\n",
    "                y_test=y_test,\n",
    "                trust_alpha=trust_alpha,\n",
    "                technique_dirs=technique_dirs,\n",
    "                use_filtering=use_filtering\n",
    "            )\n",
    "            if trust_results:\n",
    "                    results.append(trust_results)\n",
    "        elif name in [\"isotonic\", \"platt\", \"temperature\"]:\n",
    "            # Parametric calibrations\n",
    "            parametric_results = calibrate_parametric(features_val, y_val, features_test, y_test, calibrator, name, technique_dirs)\n",
    "            if parametric_results:\n",
    "                results.append(parametric_results)\n",
    "        else:\n",
    "            # Geometric calibrations\n",
    "            use_binning = calibrator.get(\"binned\", False)  # Check if binning is enabled\n",
    "            geometric_results = calibrate_geometric(\n",
    "                model, X_train, y_train, X_val, y_val, X_test, y_test, calibrator, name, technique_dirs, metric, use_binning, compression\n",
    "            )\n",
    "            if geometric_results:\n",
    "                results.append(geometric_results)\n",
    "\n",
    "    # Save all results\n",
    "    save_results(results, all_results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:34:51,379 - INFO - Loading dataset: MNIST\n",
      "2024-12-10 09:34:51,668 - INFO - Combining and splitting data\n",
      "2024-12-10 09:34:51,739 - INFO - Expanding dimensions for grayscale images\n",
      "2024-12-10 09:34:51,838 - INFO - Data normalization completed\n",
      "2024-12-10 09:34:51,842 - INFO - Using 10 epochs for training.\n",
      "2024-12-10 09:34:51,845 - INFO - Loading pre-trained RF model from MNIST/0/saved_models/RF_model.keras.\n",
      "2024-12-10 09:34:52,076 - INFO - Using sklearn model: RF\n",
      "2024-12-10 09:34:54,557 - INFO - Feature extraction complete for model type: RF\n",
      "2024-12-10 09:34:54,558 - INFO - Starting Expected Calibration Error (ECE) calculation.\n",
      "2024-12-10 09:34:54,564 - INFO - Final ECE value: 0.1657791053902575\n",
      "2024-12-10 09:34:54,564 - INFO - Calculating Maximum Calibration Error.\n",
      "2024-12-10 09:34:54,569 - INFO - Final MCE value: 0.43140644562248853\n",
      "2024-12-10 09:34:54,569 - INFO - Calculating Brier Score.\n",
      "2024-12-10 09:34:54,571 - INFO - Brier Score: 0.011209931736558482\n",
      "2024-12-10 09:34:54,571 - INFO - Calculating Log Loss.\n",
      "2024-12-10 09:34:54,580 - INFO - Calculating Average Calibration Error (ACE).\n",
      "2024-12-10 09:34:54,585 - INFO - Final ACE value: 0.2223295656591421\n",
      "2024-12-10 09:34:54,585 - INFO - Calculating Binned Likelihood.\n",
      "2024-12-10 09:34:54,590 - INFO - Final Binned Likelihood: -4244.225970238496\n",
      "2024-12-10 09:34:54,591 - INFO - Calculating CRSP.\n",
      "2024-12-10 09:34:54,593 - INFO - Final CRSP value: 0.07265364971641872\n",
      "2024-12-10 09:34:54,593 - INFO - Calculating Sharpness.\n",
      "2024-12-10 09:34:54,595 - INFO - Final Sharpness value: 0.7990066088954567\n",
      "2024-12-10 09:34:54,595 - INFO - Calculating Entropy.\n",
      "2024-12-10 09:34:54,598 - INFO - Final Entropy value: 0.707269091096669\n",
      "2024-12-10 09:34:54,599 - INFO - Calculating Negative Log-Likelihood (NLL).\n",
      "2024-12-10 09:34:54,601 - INFO - Final NLL value: 0.28945081037133324\n",
      "2024-12-10 09:34:54,601 - INFO - Initialized IsotonicCalibrator with n_classes=None, bins=15, temperature=1.0\n",
      "2024-12-10 09:34:54,602 - INFO - Initialized PlattCalibrator with n_classes=None, bins=15, temperature=1.0\n",
      "2024-12-10 09:34:54,603 - INFO - Initialized TemperatureScalingCalibrator with n_classes=None, bins=15, temperature=1.0\n",
      "2024-12-10 09:34:54,603 - INFO - Initialized TSCalibrator with temperature=1.0\n",
      "2024-12-10 09:34:54,604 - INFO - Starting parametric calibration with isotonic.\n",
      "2024-12-10 09:34:54,605 - INFO - Fitting Isotonic Calibrator.\n",
      "2024-12-10 09:34:54,607 - INFO - Validation probabilities shape: (14000, 10)\n",
      "2024-12-10 09:34:54,608 - INFO - Sample validation probabilities: [[0.00645161 0.         0.         0.00322581 0.01387097 0.\n",
      "  0.0016129  0.00645161 0.00129032 0.96709677]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.99483871 0.         0.00516129]\n",
      " [0.01382488 0.         0.02130568 0.02319508 0.00215054 0.0055914\n",
      "  0.00129032 0.00430108 0.92619048 0.00215054]\n",
      " [0.         0.         0.         0.         0.97290323 0.\n",
      "  0.         0.         0.         0.02709677]\n",
      " [0.21913978 0.00193548 0.10483871 0.34854071 0.01548387 0.14767537\n",
      "  0.04143369 0.01603687 0.0952381  0.00967742]\n",
      " [0.02075269 0.00322581 0.27177419 0.04952381 0.00903226 0.00215054\n",
      "  0.         0.50074501 0.11625192 0.02654378]\n",
      " [0.03806452 0.         0.55560676 0.16854071 0.00743472 0.03658986\n",
      "  0.03064516 0.06884793 0.07038402 0.02388633]\n",
      " [0.00752688 0.         0.85301075 0.03193548 0.02365591 0.0094086\n",
      "  0.04494624 0.00215054 0.02736559 0.        ]\n",
      " [0.0172043  0.58245776 0.10814132 0.01428571 0.00340502 0.02956989\n",
      "  0.05691756 0.00844854 0.17473118 0.00483871]\n",
      " [1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n",
      "2024-12-10 09:34:54,609 - INFO - Validation labels shape: (14000,)\n",
      "2024-12-10 09:34:54,610 - INFO - Sample validation labels: [9 7 8 4 3 2 2 2 1 0]\n",
      "2024-12-10 09:34:54,642 - INFO - Isotonic Calibrator fitted successfully.\n",
      "2024-12-10 09:34:54,643 - INFO - Starting Isotonic Calibration.\n",
      "2024-12-10 09:34:54,644 - INFO - Sample input probabilities: [[0.74182796 0.         0.02075269 0.0016129  0.02333333 0.03752688\n",
      "  0.10564516 0.         0.04564516 0.02365591]\n",
      " [0.         0.         0.00215054 0.         0.97419355 0.\n",
      "  0.00430108 0.         0.00967742 0.00967742]\n",
      " [0.         0.98225806 0.0016129  0.         0.0016129  0.0016129\n",
      "  0.         0.00645161 0.00645161 0.        ]\n",
      " [0.04400922 0.00645161 0.87324117 0.01843318 0.01666667 0.00645161\n",
      "  0.00451613 0.00898618 0.01156682 0.00967742]\n",
      " [0.02119816 0.00344086 0.028149   0.02411674 0.19926267 0.03218126\n",
      "  0.01362007 0.59142345 0.0088172  0.07779058]\n",
      " [0.01354839 0.         0.06623656 0.07109482 0.17382488 0.04127077\n",
      "  0.03       0.09792627 0.0327957  0.47330261]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.        ]\n",
      " [0.         0.84301075 0.         0.00184332 0.00645161 0.00215054\n",
      "  0.01732719 0.01044547 0.10064516 0.01812596]\n",
      " [0.         0.98032258 0.00258065 0.00903226 0.         0.\n",
      "  0.         0.0016129  0.00645161 0.        ]\n",
      " [0.         0.00376344 0.01344086 0.10963134 0.00698925 0.03193548\n",
      "  0.00537634 0.78648233 0.01053763 0.03184332]]\n",
      "2024-12-10 09:34:54,699 - INFO - Calibrated probabilities (sample): [[9.97536946e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 3.33333333e-03 0.00000000e+00\n",
      "  0.00000000e+00 4.71698113e-04]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [4.97883379e-03 4.97883379e-03 4.97883379e-03 5.47412849e-03\n",
      "  6.05343893e-02 4.97883379e-03 4.97883379e-03 8.96407405e-01\n",
      "  4.97883379e-03 7.71107422e-03]\n",
      " [6.13420971e-03 6.13420971e-03 7.45959076e-03 1.04820358e-02\n",
      "  6.16897653e-02 6.13420971e-03 6.13420971e-03 1.34334798e-02\n",
      "  6.13420971e-03 8.76264080e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  2.52525253e-03 4.71698113e-04]\n",
      " [0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.35746606e-02\n",
      "  0.00000000e+00 0.00000000e+00 0.00000000e+00 9.86486486e-01\n",
      "  0.00000000e+00 4.71698113e-04]]\n",
      "2024-12-10 09:34:54,699 - INFO - Output shape: (14000, 10)\n",
      "2024-12-10 09:34:54,700 - INFO - Starting Expected Calibration Error (ECE) calculation.\n",
      "2024-12-10 09:34:54,704 - INFO - Final ECE value: 0.01037591771964643\n",
      "2024-12-10 09:34:54,705 - INFO - Calculating Maximum Calibration Error.\n",
      "2024-12-10 09:34:54,708 - INFO - Final MCE value: 0.4156897750291344\n",
      "2024-12-10 09:34:54,709 - INFO - Calculating Brier Score.\n",
      "2024-12-10 09:34:54,711 - INFO - Brier Score: 0.005932329834901042\n",
      "2024-12-10 09:34:54,711 - INFO - Calculating Log Loss.\n",
      "/home/itayab/.conda/envs/itay_geometric/lib/python3.10/site-packages/sklearn/metrics/_classification.py:2942: UserWarning: The y_pred values do not sum to one. Make sure to pass probabilities.\n",
      "  warnings.warn(\n",
      "2024-12-10 09:34:54,719 - INFO - Calculating Average Calibration Error (ACE).\n",
      "2024-12-10 09:34:54,722 - INFO - Final ACE value: 0.11507177040694683\n",
      "2024-12-10 09:34:54,723 - INFO - Calculating Binned Likelihood.\n",
      "2024-12-10 09:34:54,726 - INFO - Final Binned Likelihood: -4589.9347906049425\n",
      "2024-12-10 09:34:54,727 - INFO - Calculating CRSP.\n",
      "2024-12-10 09:34:54,729 - INFO - Final CRSP value: 0.02738682905751657\n",
      "2024-12-10 09:34:54,729 - INFO - Calculating Sharpness.\n",
      "2024-12-10 09:34:54,731 - INFO - Final Sharpness value: 0.9608507413245141\n",
      "2024-12-10 09:34:54,732 - INFO - Calculating Entropy.\n",
      "2024-12-10 09:34:54,735 - INFO - Final Entropy value: 0.15755210124578828\n",
      "2024-12-10 09:34:54,735 - INFO - Calculating Negative Log-Likelihood (NLL).\n",
      "2024-12-10 09:34:54,737 - INFO - Final NLL value: 0.11310960789486746\n",
      "2024-12-10 09:34:54,741 - INFO - Starting parametric calibration with platt.\n",
      "2024-12-10 09:34:54,742 - INFO - Fitting Platt Calibrator.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncalibrated Metrics: {'ECE': 0.1657791053902575, 'MCE': 0.43140644562248853, 'Brier Score': 0.011209931736558482, 'Log Loss': 0.28945081037284176, 'ACE': 0.2223295656591421, 'Binned Likelihood': -4244.225970238496, 'CRSP': 0.07265364971641872, 'Sharpness': 0.7990066088954567, 'Entropy': 0.707269091096669, 'NLL': 0.28945081037133324}\n",
      "Isotonic Metrics saved to /cs/cs_groups/cliron_group/Calibrato/MNIST/0/RF/cosine/isotonic/results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:34:54,878 - INFO - Platt Calibrator fitted successfully for all classes.\n",
      "2024-12-10 09:34:54,879 - INFO - Calibrating probabilities using Platt Scaling.\n",
      "2024-12-10 09:34:54,885 - INFO - Starting Expected Calibration Error (ECE) calculation.\n",
      "2024-12-10 09:34:54,889 - INFO - Final ECE value: 0.03702463707063028\n",
      "2024-12-10 09:34:54,889 - INFO - Calculating Maximum Calibration Error.\n",
      "2024-12-10 09:34:54,893 - INFO - Final MCE value: 0.2472228804939257\n",
      "2024-12-10 09:34:54,893 - INFO - Calculating Brier Score.\n",
      "2024-12-10 09:34:54,895 - INFO - Brier Score: 0.00556554930124345\n",
      "2024-12-10 09:34:54,895 - INFO - Calculating Log Loss.\n",
      "2024-12-10 09:34:54,904 - INFO - Calculating Average Calibration Error (ACE).\n",
      "2024-12-10 09:34:54,907 - INFO - Final ACE value: 0.06301385033090981\n",
      "2024-12-10 09:34:54,908 - INFO - Calculating Binned Likelihood.\n",
      "2024-12-10 09:34:54,912 - INFO - Final Binned Likelihood: -4358.335622419387\n",
      "2024-12-10 09:34:54,913 - INFO - Calculating CRSP.\n",
      "2024-12-10 09:34:54,914 - INFO - Final CRSP value: 0.026213359676835832\n",
      "2024-12-10 09:34:54,915 - INFO - Calculating Sharpness.\n",
      "2024-12-10 09:34:54,917 - INFO - Final Sharpness value: 0.9287283386087906\n",
      "2024-12-10 09:34:54,917 - INFO - Calculating Entropy.\n",
      "2024-12-10 09:34:54,920 - INFO - Final Entropy value: 0.3116621458460409\n",
      "2024-12-10 09:34:54,920 - INFO - Calculating Negative Log-Likelihood (NLL).\n",
      "2024-12-10 09:34:54,922 - INFO - Final NLL value: 0.14055544944722576\n",
      "2024-12-10 09:34:54,926 - INFO - Starting parametric calibration with temperature.\n",
      "2024-12-10 09:34:54,926 - INFO - TemperatureScalingCalibrator: Starting the fitting process.\n",
      "2024-12-10 09:34:54,927 - INFO - TemperatureScalingCalibrator: Beginning optimization.\n",
      "2024-12-10 09:34:55,120 - INFO - TemperatureScalingCalibrator: Convergence reached.\n",
      "2024-12-10 09:34:55,122 - INFO - TemperatureScalingCalibrator: Fitting complete. Final temperature: 0.08185058832168579\n",
      "2024-12-10 09:34:55,124 - INFO - TemperatureScalingCalibrator: Calibrating probabilities.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platt Metrics saved to /cs/cs_groups/cliron_group/Calibrato/MNIST/0/RF/cosine/platt/results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:34:55,136 - INFO - Starting Expected Calibration Error (ECE) calculation.\n",
      "2024-12-10 09:34:55,142 - INFO - Final ECE value: 0.027499796641780375\n",
      "2024-12-10 09:34:55,144 - INFO - Calculating Maximum Calibration Error.\n",
      "2024-12-10 09:34:55,150 - INFO - Final MCE value: 0.5611157343777573\n",
      "2024-12-10 09:34:55,152 - INFO - Calculating Brier Score.\n",
      "2024-12-10 09:34:55,155 - INFO - Brier Score: 0.0061748994326638095\n",
      "2024-12-10 09:34:55,157 - INFO - Calculating Log Loss.\n",
      "2024-12-10 09:34:55,170 - INFO - Calculating Average Calibration Error (ACE).\n",
      "2024-12-10 09:34:55,175 - INFO - Final ACE value: 0.14901710344154728\n",
      "2024-12-10 09:34:55,179 - INFO - Calculating Binned Likelihood.\n",
      "2024-12-10 09:34:55,186 - INFO - Final Binned Likelihood: -8026.865556498447\n",
      "2024-12-10 09:34:55,188 - INFO - Calculating CRSP.\n",
      "2024-12-10 09:34:55,191 - INFO - Final CRSP value: 0.03009619272019163\n",
      "2024-12-10 09:34:55,193 - INFO - Calculating Sharpness.\n",
      "2024-12-10 09:34:55,196 - INFO - Final Sharpness value: 0.9923128387390922\n",
      "2024-12-10 09:34:55,199 - INFO - Calculating Entropy.\n",
      "2024-12-10 09:34:55,205 - INFO - Final Entropy value: 0.018384589246595025\n",
      "2024-12-10 09:34:55,207 - INFO - Calculating Negative Log-Likelihood (NLL).\n",
      "2024-12-10 09:34:55,209 - INFO - Final NLL value: 0.28084768811127525\n",
      "2024-12-10 09:34:55,217 - INFO - Starting Geometric Trust Score calibration with binning: True\n",
      "2024-12-10 09:34:55,220 - ERROR - Error during Geometric Trust Score calibration: GeometricCalibratorTrust.__init__() got an unexpected keyword argument 'use_filtering'\n",
      "2024-12-10 09:34:55,222 - INFO - Initializing StabilitySpace with faiss library and cosine metric.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature Metrics saved to /cs/cs_groups/cliron_group/Calibrato/MNIST/0/RF/cosine/temperature/results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 09:34:56,337 - INFO - Initialized GeometricCalibrator with n_classes=None, bins=15, temperature=1.0\n",
      "2024-12-10 09:34:56,340 - INFO - GeometricCalibrator: Using custom StabilitySpace provided by user.\n",
      "2024-12-10 09:34:56,340 - INFO - Initialized GeometricCalibrator with model RandomForestClassifier and fitting function IsotonicRegression.\n",
      "2024-12-10 09:34:56,341 - INFO - GeometricCalibrator: Fitting with validation data using balanced accuracy and rounded stability.\n",
      "2024-12-10 09:34:56,341 - INFO - Sklearn model detected\n",
      "2024-12-10 09:34:57,595 - INFO - Calculating stability using faiss.\n",
      "2024-12-10 09:34:57,596 - INFO - Calculating stability using FAISS.\n",
      "Calculating Stability (FAISS):  14%|ââ        | 1915/14000 [00:21<02:18, 87.31sample/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_858011/74465806.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MNIST\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"RF\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cosine\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_858011/3982475459.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(dataset_name, random_state, model_type, metric, transformed, trust_alpha, compression_types, compression_params)\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparametric_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;31m# Geometric calibrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0muse_binning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"binned\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Check if binning is enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             geometric_results = calibrate_geometric(\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalibrator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtechnique_dirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_binning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             )\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgeometric_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_858011/3439915154.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, X_train, y_train, X_val, y_val, X_test, y_test, calibrator, name, technique_dirs, metric, use_binning, compression)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mresults_csv_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtechnique_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_csv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{name.replace('_', ' ').capitalize()} Metrics saved to {results_csv_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error calibrating with {name}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/cs_groups/cliron_group/Calibrato/calibrators/geometric_calibrators.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X_val, y_val)\u001b[0m\n",
      "\u001b[0;32m/cs/cs_groups/cliron_group/Calibrato/utils/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, X_val, y_val_pred, timeout)\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Calculating stability using {self.library}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Start timing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'faiss'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m             \u001b[0mstability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stability_faiss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'knn'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0mstability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stability_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'separation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cs/cs_groups/cliron_group/Calibrato/utils/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, valX, val_y_pred)\u001b[0m\n\u001b[1;32m    600\u001b[0m                     \u001b[0mdist_same\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist_same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                     \u001b[0mdist_other\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist_other\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mstability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdist_other\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdist_same\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error in FAISS stability calculation for sample {i}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mstability\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/itay_geometric/lib/python3.10/site-packages/faiss/class_wrappers.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, k, params, D, I)\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/itay_geometric/lib/python3.10/site-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, n, x, k, distances, labels, params)\u001b[0m\n\u001b[1;32m   2276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFlat_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(dataset_name=\"MNIST\", random_state=0, model_type=\"RF\", metric=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itay_geometric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
